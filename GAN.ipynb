{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 2246)\n",
      "(?, 2205)\n",
      "(?, 60)\n",
      "(?, 5)\n",
      "(?, 2240)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['discriminator_1/dense_6/kernel:0', 'discriminator_1/dense_6/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator/dense_1/kernel:0', 'generator/dense_1/bias:0', 'generator/dense_2/kernel:0', 'generator/dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"concat:0\", shape=(80, 60), dtype=float32) which was passed to the argument `feed_dict` with key Tensor(\"noise_img:0\", shape=(?, 60), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 228\u001b[0m\n\u001b[1;32m    221\u001b[0m             real_image_batch_np \u001b[39m=\u001b[39m sess1\u001b[39m.\u001b[39mrun(tf\u001b[39m.\u001b[39midentity(real_image_batch))\n\u001b[1;32m    223\u001b[0m         \u001b[39m#print(real_img.shape, real_image_batch.shape)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m         \u001b[39m# Run optimizers\u001b[39;00m\n\u001b[1;32m    225\u001b[0m         \u001b[39m#real_image_batch_tensor = tf.convert_to_tensor(real_image_batch)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[39m#label_batch_tensor = tf.convert_to_tensor(label_batch)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         \u001b[39m#batch_noise_tensor = tf.convert_to_tensor(batch_noise, dtype=tf.float32)\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m         sess\u001b[39m.\u001b[39;49mrun(d_train_opt, feed_dict\u001b[39m=\u001b[39;49m{real_img: real_image_batch_np, noise_img: batch_noise_tensor, ground_truth: label_batch_tensor})\n\u001b[1;32m    229\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m        for i in range(2):\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m            _ = tf.compat.v1.keras.backend.get_session().run(g_train_opt,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m \n\u001b[1;32m    290\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/williamblair/miniconda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/williamblair/miniconda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/client/session.py:1139\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1136\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot interpret feed_dict key as Tensor: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(subfeed_val, ops\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m-> 1139\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1140\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe value of a feed cannot be a tf.Tensor object. Acceptable \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1141\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mfeed values include Python scalars, strings, lists, numpy \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1142\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mndarrays, or TensorHandles. For reference, the tensor object \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1143\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwas \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(feed_val)\u001b[39m}\u001b[39;00m\u001b[39m which was passed to the argument \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1144\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`feed_dict` with key \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(feed)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1146\u001b[0m subfeed_dtype \u001b[39m=\u001b[39m subfeed_t\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mas_numpy_dtype\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(subfeed_val, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m _convert_to_numpy_obj(\n\u001b[1;32m   1148\u001b[0m     subfeed_dtype, subfeed_val) \u001b[39m!=\u001b[39m subfeed_val:\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"concat:0\", shape=(80, 60), dtype=float32) which was passed to the argument `feed_dict` with key Tensor(\"noise_img:0\", shape=(?, 60), dtype=float32)."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def one_hot(y_):\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    y_ = [int(x) for x in y_]\n",
    "    n_values = np.max(y_) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]\n",
    "\n",
    "def get_generator(noise_img, n_units, out_dim, reuse=False, alpha=0.01):\n",
    "    global n_class\n",
    "    momentum = 0.9\n",
    "\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        # Deconvolutional\n",
    "        noise_img = tf.keras.layers.Dense(units=140 * 64)(noise_img)\n",
    "\n",
    "        h0 = tf.reshape(noise_img, [-1, 10, 14, 64])\n",
    "\n",
    "        h0 = tf.nn.relu(tf.keras.layers.BatchNormalization(momentum=momentum)(h0))\n",
    "\n",
    "        h3 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same')(h0)\n",
    "        h3 = tf.nn.relu(tf.keras.layers.BatchNormalization(momentum=momentum)(h3))\n",
    "\n",
    "        h4 = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same', name='g')(h3)\n",
    "\n",
    "        # logits & outputs\n",
    "        logits = tf.reshape(h4, [-1, 40 * 56])\n",
    "        outputs = tf.math.tanh(logits)\n",
    "        mid = tf.math.sigmoid(tf.keras.layers.Dense(units=20 * 20)(logits))\n",
    "        pred = tf.keras.layers.Dense(units=n_class)(mid)\n",
    "\n",
    "        return logits, outputs, pred\n",
    "\n",
    "def get_discriminator(img, n_units, reuse=False, alpha=0.01, cond=None):\n",
    "    momentum = 0.9\n",
    "    con = cond[:, noise_size - 40:noise_size]\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        # CNN\n",
    "        z_image = tf.reshape(img, [-1, 40, 56, 1])\n",
    "        h0 = lrelu(tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=2, padding='same')(z_image))\n",
    "        h0 = lrelu(tf.keras.layers.BatchNormalization(momentum=momentum)(h0))\n",
    "\n",
    "        h1 = lrelu(tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same')(h0))\n",
    "        h1 = lrelu(tf.keras.layers.BatchNormalization(momentum=momentum)(h1))\n",
    "\n",
    "        hidden1 = tf.keras.layers.Flatten()(h1)\n",
    "\n",
    "        hidden1 = tf.concat([hidden1, con], axis=1)\n",
    "\n",
    "        logits = tf.keras.layers.Dense(units=1)(hidden1)\n",
    "        outputs = logits\n",
    "        outputs_2 = tf.keras.layers.Dense(units=n_class)(hidden1)\n",
    "\n",
    "        return logits, outputs, outputs_2\n",
    "    \n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    correct_prediction = tf.equal(tf.argmax(v_xs, 1), tf.argmax(v_ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    return tf.maximum(x, leak * x)\n",
    "\n",
    "img_size = 2205\n",
    "noise_size = 60\n",
    "n_class = 5\n",
    "\n",
    "\"\"\"\n",
    "load the shuffled data, [3200, 2281], 2286 = 40 + 2240 + 1\n",
    "In which, 40 is EEG features, 2240 = 40*56 is the image, 1 is the label\n",
    "\"\"\"\n",
    "data = pickle.load(open('shape_EEG_feature.pkl', 'rb'))\n",
    "print(data.shape)\n",
    "\n",
    "label = data[:, -1]\n",
    "label.shape = [3200, 1]\n",
    "# print(np.sum(label), label.shape, np.max(data[0, 40:-1]), np.min(data[0, 40:-1]))\n",
    "\n",
    "label = tf.one_hot(label, n_class)\n",
    "\n",
    "# make the D_2 label\n",
    "g_units = 200\n",
    "d_units = g_units\n",
    "alpha = 0.01\n",
    "# label smoothing\n",
    "smooth = 0.1\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "real_img = tf.compat.v1.placeholder(tf.float32, [None, img_size], name='real_img')\n",
    "noise_img = tf.compat.v1.placeholder(tf.float32, [None, noise_size], name='noise_img')\n",
    "ground_truth = tf.compat.v1.placeholder(tf.float32, shape=[None, n_class], name='ground_truth')\n",
    "print(real_img.shape)\n",
    "print(noise_img.shape)\n",
    "print(ground_truth.shape)\n",
    "\n",
    "real_img_reshaped = tf.reshape(real_img, [-1, 2240])\n",
    "print(real_img_reshaped.shape)\n",
    "\n",
    "# generator\n",
    "g_logits, g_outputs, pred = get_generator(noise_img, g_units, img_size)\n",
    "# discriminator\n",
    "d_logits_real, d_outputs_real, real_category_pred = get_discriminator(real_img, d_units, cond=noise_img)\n",
    "d_logits_fake, d_outputs_fake, fake_category_pred = get_discriminator(g_outputs, d_units, cond=noise_img, reuse=True)\n",
    "\n",
    "# ACC\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(ground_truth, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# # discriminatorloss # cross-entropy\n",
    "d_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)) * (1 - smooth))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "\n",
    "# reduce_mean\n",
    "d_loss_rf = d_loss_real + d_loss_fake\n",
    "\n",
    "d_loss_category_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=real_category_pred, labels=ground_truth))\n",
    "d_loss_category_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_category_pred, labels=ground_truth))\n",
    "d_loss_category = 0.8 * d_loss_category_real + d_loss_category_fake\n",
    "d_loss = d_loss_rf + d_loss_category_real\n",
    "\n",
    "# classifier loss\n",
    "c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=ground_truth))\n",
    "# calculate the inception classification accuracy, evaluating is the generated image is correct?\n",
    "IC_fake = compute_accuracy(ground_truth, fake_category_pred)\n",
    "IC_real = compute_accuracy(ground_truth, real_category_pred)\n",
    "\n",
    "# reshape the array\n",
    "#arr.reshape((-1, 2400))\n",
    "\n",
    "# generator loss\n",
    "lambda_ = 0.01\n",
    "batch_size = 80\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)) * (1 - smooth))\n",
    "\n",
    "# regularization of generated fake image with the real image:\n",
    "g_regular = tf.keras.losses.mean_squared_error(y_true=real_img_reshaped, y_pred=g_outputs)\n",
    "g_loss = g_loss + d_loss_category_fake + lambda_ * g_regular\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "# generator tensor\n",
    "g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n",
    "# discriminator tensor\n",
    "d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0002  # 0.0002\n",
    "d_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "g_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "c_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# batch_size\n",
    "epochs = 200\n",
    "n_sample = batch_size\n",
    "n_batch = int(data.shape[0] / batch_size)\n",
    "\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Define discriminator optimizer\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "d_gradients = tf.gradients(d_loss, d_vars)\n",
    "d_train_opt = d_optimizer.apply_gradients(zip(d_gradients, d_vars))\n",
    "\n",
    "# Define generator optimizer\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "g_gradients = tf.gradients(g_loss, g_vars)\n",
    "g_train_opt = g_optimizer.apply_gradients(zip(g_gradients, g_vars))\n",
    "\n",
    "# Initialize global variables\n",
    "tf.compat.v1.global_variables_initializer().run(session=sess)\n",
    "\n",
    "for e in range(epochs):\n",
    "    for h in range(n_batch):\n",
    "        # print(\"this is number \", h)\n",
    "        z_batch = data[batch_size * h:batch_size * (h + 1), 0:40]\n",
    "        real_image_batch = data[batch_size * h:batch_size * (h + 1), 40:-1]\n",
    "        \n",
    "        label_batch = label[batch_size * h:batch_size * (h + 1)]\n",
    "        label_batch = tf.cast(label_batch, tf.float32)\n",
    "        label_batch_tensor = tf.convert_to_tensor(label_batch)\n",
    "        label_batch_tensor = tf.cast(label_batch_tensor, dtype=tf.float32)\n",
    "\n",
    "        # batch_images = batch[0].reshape((batch_size, 784))\n",
    "        real_image_batch = real_image_batch * 2 - 1\n",
    "        z_batch = z_batch * 2 - 1\n",
    "\n",
    "        # reshape the real_image_batch to (?, 2240)\n",
    "        #real_image_batch = tf.reshape(real_image_batch, (-1, 2240))\n",
    "\n",
    "        batch_noise = tf.random.uniform([batch_size, noise_size], minval=-1, maxval=1)\n",
    "        batch_noise = tf.concat([batch_noise[:, :20], z_batch[:, :40]], axis=1)  # 130 noise + 10 EEG channels\n",
    "        # batch_noise = z_batch[:, :40]\n",
    "        batch_noise_tensor = tf.cast(batch_noise, dtype=tf.float32)\n",
    "\n",
    "        # Convert real_image_batch to a NumPy ndarray\n",
    "        with tf.compat.v1.Session() as sess1:\n",
    "            real_image_batch_np = sess1.run(tf.identity(real_image_batch))\n",
    "\n",
    "        #print(real_img.shape, real_image_batch.shape)\n",
    "        # Run optimizers\n",
    "        #real_image_batch_tensor = tf.convert_to_tensor(real_image_batch)\n",
    "        #label_batch_tensor = tf.convert_to_tensor(label_batch)\n",
    "        #batch_noise_tensor = tf.convert_to_tensor(batch_noise, dtype=tf.float32)\n",
    "        sess.run(d_train_opt, feed_dict={real_img: real_image_batch_np, noise_img: batch_noise_tensor, ground_truth: label_batch_tensor})\n",
    "    \"\"\"\n",
    "        for i in range(2):\n",
    "            _ = tf.compat.v1.keras.backend.get_session().run(g_train_opt,\n",
    "                                                            feed_dict={real_img: real_image_batch, noise_img: batch_noise, ground_truth: label_batch})\n",
    "\n",
    "    if e % 50 == 0 and e != 0:\n",
    "        # discriminator loss\n",
    "        train_loss_d, train_loss_d_rf, train_loss_d_category = tf.compat.v1.Session().run([d_loss, d_loss_rf, d_loss_category],\n",
    "                                                                        feed_dict={real_img: real_image_batch,\n",
    "                                                                                   noise_img: batch_noise,\n",
    "                                                                                   ground_truth: label_batch})\n",
    "\n",
    "        # generator loss\n",
    "        train_loss_g, train_loss_c, acc, g_regular_ = tf.compat.v1.Session().run([g_loss, c_loss, accuracy, g_regular],\n",
    "                                                               feed_dict={real_img: real_image_batch,\n",
    "                                                                          noise_img: batch_noise,\n",
    "                                                                          ground_truth: label_batch})\n",
    "        # IC score\n",
    "        ic_real_, ic_fake_ = tf.compat.v1.Session().run([IC_real, IC_fake],\n",
    "                                      feed_dict={real_img: real_image_batch, noise_img: batch_noise,\n",
    "                                                 ground_truth: label_batch})\n",
    "        print(\"Epoch {}/{}\".format(e + 1, epochs),\n",
    "              \"D Loss: {:.4f}(r/f: {:.4f} + category: {:.4f})\".format(train_loss_d, train_loss_d_rf,\n",
    "                                                                      train_loss_d_category),\n",
    "              \"G Loss: {:.4f}, RMSE:{:.4f} , C loss{:.4f}, acc, {:.4f}\".format(train_loss_g,\n",
    "                                                                    lambda_ * g_regular_, train_loss_c, acc),\n",
    "              'IC real.{:.4f}, fake: {:.4f}'.format(ic_real_, ic_fake_))\n",
    "        losses.append((train_loss_d, train_loss_d_rf, train_loss_d_category, train_loss_g), )\n",
    "\n",
    "        # fig = plt.figure(figsize=(30, 6))\n",
    "        # print 'true label,', true_label[1:10]\n",
    "        # no_pic = 12\n",
    "        # for i in range(1, no_pic+1):  # 8 samples including 5 categories\n",
    "        #     generated_image = gen_samples[i].reshape([40, 56])\n",
    "        #     real_image = real_[i].reshape([40, 56])\n",
    "        #     fig.add_subplot(1, no_pic, i)\n",
    "        #     plt.axis('off')\n",
    "        #     plt.imshow(generated_image, cmap='gray_r')\n",
    "        # plt.savefig('generated_images/step'+str(e)+'.png', format='png', bbox_inches='tight')\n",
    "        # pickle.dump(gen_samples, open('GAN_1.pk', 'wb'))\n",
    "\n",
    "        #Test\n",
    "        h = n_batch - 3\n",
    "        z_batch_ = data[batch_size * h:batch_size * (h + 1), :40]  # the last batch worked as testingsample\n",
    "        true_label = data[batch_size * h:batch_size * (h + 1), -1]\n",
    "        real_ = data[batch_size * h:batch_size * (h + 1), 40:-1]\n",
    "        #half noise half EEG\n",
    "        sample_noise = np.random.uniform(-1, 1, size=(n_sample, noise_size))\n",
    "        sample_noise = np.hstack((sample_noise[:, :20], z_batch_[:, :40]))  # 130 noise + 10 EEG channels\n",
    "\n",
    "        with tf.compat.v1.variable_scope('generator', reuse=True):\n",
    "            w1 = tf.compat.v1.get_variable('w1')\n",
    "            b1 = tf.compat.v1.get_variable('b1')\n",
    "            w2 = tf.compat.v1.get_variable('w2')\n",
    "            b2 = tf.compat.v1.get_variable('b2')\n",
    "            hidden = tf.nn.relu(tf.matmul(noise_img, w1) + b1)\n",
    "            output = tf.nn.tanh(tf.matmul(hidden, w2) + b2)\n",
    "\n",
    "        _, gen_samples, pred_ = tf.compat.v1.Session().run((w1, w2, b1, b2, output),\n",
    "                                         feed_dict={real_img: real_, noise_img: sample_noise, })\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
